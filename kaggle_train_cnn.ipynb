{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BeanScan CNN Training on Kaggle\n",
        "\n",
        "This notebook is ready to run on Kaggle. Attach your datasets via the right sidebar:\n",
        "- Your code as a Kaggle Dataset containing the `backend/ml` folder\n",
        "- Your images/annotations dataset\n",
        "- (Optional) A weights dataset to resume training\n",
        "\n",
        "Then set the parameters below and run all cells (GPU recommended).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Parameters (edit these) ===\n",
        "DATASET_IMAGES = \"/kaggle/input/your-images-dataset\"  # folder with images and *_annotations.json\n",
        "DATASET_CODE = \"/kaggle/input/beanscan-ml-code\"       # dataset containing backend/ml and backend/utils\n",
        "DATASET_WEIGHTS = None  # e.g., \"/kaggle/input/beanscan-weights\" or None\n",
        "WEIGHTS_FILE = None     # e.g., \"/kaggle/input/cnn-cnn-v1/cnn_best.pth\" when attaching a Kaggle Model\n",
        "\n",
        "# Training options\n",
        "USE_GPU = True\n",
        "NUM_EPOCHS = 20\n",
        "BATCH_SIZE = 32\n",
        "SAVE_INTERVAL = 5\n",
        "MODELS_DIR = \"/kaggle/working/models\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Environment setup ===\n",
        "# Install torch/torchvision compatible with Kaggle CUDA (usually 11.8)\n",
        "%pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "# Optional: if you uploaded your repo requirements as a file, you can point to it\n",
        "# Here we install typical dependencies used by your training code\n",
        "%pip install -q numpy pandas matplotlib tqdm pillow opencv-python scikit-learn albumentations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Inline backend/ml sources so no code dataset is needed ===\n",
        "import os\n",
        "base_dir = \"/kaggle/working/backend/ml\"\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "custom_models_src = r'''import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import mobilenet_v3_small, mobilenet_v3_large\n",
        "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
        "from torchvision.models.detection import fasterrcnn_mobilenet_v3_large_fpn\n",
        "from torchvision.models.detection.backbone_utils import BackboneWithFPN\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "import torchvision.transforms as transforms\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import numpy as np\n",
        "\n",
        "class MobileNetV3Backbone(nn.Module):\n",
        "    \"\"\"Custom MobileNetV3 backbone for feature extraction\"\"\"\n",
        "    \n",
        "    def __init__(self, pretrained: bool = True, width_mult: float = 1.0):\n",
        "        super().__init__()\n",
        "        # Load pretrained MobileNetV3\n",
        "        if pretrained:\n",
        "            self.backbone = mobilenet_v3_small(pretrained=True)\n",
        "        else:\n",
        "            self.backbone = mobilenet_v3_small(pretrained=False)\n",
        "        \n",
        "        # Extract features from different layers\n",
        "        self.features = self.backbone.features\n",
        "        \n",
        "        # Feature dimensions for different scales\n",
        "        self.feature_channels = [16, 24, 40, 48, 96, 576]\n",
        "        \n",
        "    def forward(self, x):\n",
        "        features = []\n",
        "        for i, layer in enumerate(self.features):\n",
        "            x = layer(x)\n",
        "            if i in [2, 4, 6, 8, 10, 12]:  # Key feature layers\n",
        "                features.append(x)\n",
        "        return features\n",
        "\n",
        "class BeanClassifierCNN(nn.Module):\n",
        "    \"\"\"CNN for bean type classification using MobileNetV3 backbone\"\"\"\n",
        "    \n",
        "    def __init__(self, num_classes: int = 4, pretrained: bool = True):\n",
        "        super().__init__()\n",
        "        self.backbone = MobileNetV3Backbone(pretrained=pretrained)\n",
        "        \n",
        "        # Classification head (increased dropout ~0.3 to mitigate overfitting)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(576, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "        \n",
        "        # Bean type names\n",
        "        self.class_names = [\"Arabica\", \"Robusta\", \"Liberica\", \"Excelsa\"]\n",
        "        \n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)\n",
        "        # Use the last feature map for classification\n",
        "        x = features[-1]\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "    \n",
        "    def predict(self, x, threshold: float = 0.5):\n",
        "        \"\"\"Predict bean type with confidence\"\"\"\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            logits = self.forward(x)\n",
        "            probabilities = F.softmax(logits, dim=1)\n",
        "            confidence, predicted = torch.max(probabilities, 1)\n",
        "            \n",
        "            # Filter by confidence threshold\n",
        "            mask = confidence >= threshold\n",
        "            predictions = []\n",
        "            \n",
        "            for i in range(len(predicted)):\n",
        "                if mask[i]:\n",
        "                    predictions.append({\n",
        "                        'class': self.class_names[predicted[i].item()],\n",
        "                        'confidence': confidence[i].item(),\n",
        "                        'probabilities': probabilities[i].tolist()\n",
        "                    })\n",
        "                else:\n",
        "                    predictions.append({\n",
        "                        'class': 'Unknown',\n",
        "                        'confidence': confidence[i].item(),\n",
        "                        'probabilities': probabilities[i].tolist()\n",
        "                    })\n",
        "            \n",
        "            return predictions\n",
        "\n",
        "class DefectDetectorMaskRCNN(nn.Module):\n",
        "    \"\"\"Mask R-CNN for defect detection using MobileNetV3 backbone\"\"\"\n",
        "    \n",
        "    def __init__(self, num_classes: int = 4, pretrained: bool = True):\n",
        "        super().__init__()\n",
        "        # Create custom backbone with MobileNetV3\n",
        "        self.backbone = MobileNetV3Backbone(pretrained=pretrained)\n",
        "        \n",
        "        # Create FPN from backbone features\n",
        "        self.fpn = BackboneWithFPN(\n",
        "            self.backbone,\n",
        "            return_layers={'0': '0', '1': '1', '2': '2', '3': '3', '4': '4', '5': '5'},\n",
        "            in_channels_list=[16, 24, 40, 48, 96, 576],\n",
        "            out_channels=256\n",
        "        )\n",
        "        \n",
        "        # Create Mask R-CNN with custom backbone\n",
        "        self.mask_rcnn = maskrcnn_resnet50_fpn(\n",
        "            pretrained=False,\n",
        "            num_classes=num_classes + 1  # +1 for background\n",
        "        )\n",
        "        \n",
        "        # Replace backbone\n",
        "        self.mask_rcnn.backbone = self.fpn\n",
        "        \n",
        "        # Customize box and mask predictors\n",
        "        in_features = self.mask_rcnn.roi_heads.box_predictor.cls_score.in_features\n",
        "        self.mask_rcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes + 1)\n",
        "        \n",
        "        in_features_mask = self.mask_rcnn.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "        hidden_layer = 256\n",
        "        self.mask_rcnn.roi_heads.mask_predictor = MaskRCNNPredictor(\n",
        "            in_features_mask, hidden_layer, num_classes + 1\n",
        "        )\n",
        "        \n",
        "        # Defect types\n",
        "        self.defect_types = [\"Mold\", \"Insect_Damage\", \"Discoloration\", \"Physical_Damage\"]\n",
        "        \n",
        "    def forward(self, images, targets=None):\n",
        "        return self.mask_rcnn(images, targets)\n",
        "\n",
        "class DefectDetectorFasterRCNN(nn.Module):\n",
        "    \"\"\"Faster R-CNN detector (bounding boxes only) for bean defects\"\"\"\n",
        "    \n",
        "    def __init__(self, num_classes: int = 7, pretrained: bool = True,\n",
        "                 class_names: Optional[List[str]] = None):\n",
        "        super().__init__()\n",
        "        # num_classes should include background (>=2)\n",
        "        self.num_classes = max(2, num_classes)\n",
        "        self.model = fasterrcnn_mobilenet_v3_large_fpn(pretrained=pretrained)\n",
        "        in_features = self.model.roi_heads.box_predictor.cls_score.in_features\n",
        "        self.model.roi_heads.box_predictor = FastRCNNPredictor(in_features, self.num_classes)\n",
        "        \n",
        "        default_classes = [\n",
        "            \"insect_damage\",\n",
        "            \"nugget\",\n",
        "            \"quaker\",\n",
        "            \"roasted-beans\",\n",
        "            \"shell\",\n",
        "            \"under_roast\"\n",
        "        ]\n",
        "        self.class_names = [\"__background__\"] + (class_names or default_classes)\n",
        "    \n",
        "    def forward(self, images, targets=None):\n",
        "        return self.model(images, targets)\n",
        "\n",
        "class ShelfLifeLSTM(nn.Module):\n",
        "    \"\"\"LSTM for shelf life prediction based on defect progression\"\"\"\n",
        "    \n",
        "    def __init__(self, input_size: int = 64, hidden_size: int = 128, num_layers: int = 2, dropout: float = 0.2):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        # LSTM layers\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        \n",
        "        # Attention mechanism\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            embed_dim=hidden_size * 2,  # *2 for bidirectional\n",
        "            num_heads=8,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        \n",
        "        # Prediction head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_size * 2, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size, hidden_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size // 2, 1)  # Predict days until expiration\n",
        "        )\n",
        "        \n",
        "        # Shelf life categories\n",
        "        self.shelf_life_categories = [\"Expired\", \"Critical\", \"Warning\", \"Good\", \"Excellent\"]\n",
        "        \n",
        "    def forward(self, x, hidden=None):\n",
        "        # x shape: (batch_size, seq_len, input_size)\n",
        "        batch_size = x.size(0)\n",
        "        \n",
        "        # Initialize hidden state if not provided\n",
        "        if hidden is None:\n",
        "            h0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size).to(x.device)\n",
        "            c0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size).to(x.device)\n",
        "            hidden = (h0, c0)\n",
        "        \n",
        "        # LSTM forward pass\n",
        "        lstm_out, hidden = self.lstm(x, hidden)\n",
        "        \n",
        "        # Apply attention\n",
        "        lstm_out = lstm_out.transpose(0, 1)  # (seq_len, batch_size, hidden_size*2)\n",
        "        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
        "        attn_out = attn_out.transpose(0, 1)  # (batch_size, seq_len, hidden_size*2)\n",
        "        \n",
        "        # Global average pooling\n",
        "        pooled = torch.mean(attn_out, dim=1)  # (batch_size, hidden_size*2)\n",
        "        \n",
        "        # Predict shelf life\n",
        "        shelf_life = self.classifier(pooled)\n",
        "        \n",
        "        return shelf_life, hidden\n",
        "\n",
        "class BeanScanEnsemble(nn.Module):\n",
        "    \"\"\"Ensemble model combining CNN, Mask R-CNN, and LSTM\"\"\"\n",
        "    \n",
        "    def __init__(self, cnn_model: BeanClassifierCNN, \n",
        "                 defect_model: DefectDetectorMaskRCNN,\n",
        "                 lstm_model: ShelfLifeLSTM):\n",
        "        super().__init__()\n",
        "        self.cnn_model = cnn_model\n",
        "        self.defect_model = defect_model\n",
        "        self.lstm_model = lstm_model\n",
        "        \n",
        "    def forward(self, image, defect_sequence=None):\n",
        "        \"\"\"Complete bean analysis pipeline\"\"\"\n",
        "        results = {}\n",
        "        \n",
        "        # 1. Bean type classification\n",
        "        bean_type = self.cnn_model.predict(image)\n",
        "        results['bean_classification'] = bean_type\n",
        "        \n",
        "        # 2. Defect detection\n",
        "        defects = self.defect_model.detect_defects(image)\n",
        "        results['defect_detection'] = defects\n",
        "        \n",
        "        # 3. Shelf life prediction (if sequence provided)\n",
        "        if defect_sequence is not None:\n",
        "            shelf_life = self.lstm_model.predict_shelf_life(defect_sequence)\n",
        "            results['shelf_life_prediction'] = shelf_life\n",
        "        \n",
        "        # 4. Calculate overall health score\n",
        "        health_score = self._calculate_health_score(bean_type, defects)\n",
        "        results['health_score'] = health_score\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def _calculate_health_score(self, bean_type, defects):\n",
        "        \"\"\"Calculate overall bean health score\"\"\"\n",
        "        # Base score from bean type confidence\n",
        "        base_score = bean_type[0]['confidence'] if bean_type else 0.5\n",
        "        \n",
        "        # Penalty for defects\n",
        "        defect_penalty = 0\n",
        "        if defects:\n",
        "            for defect in defects:\n",
        "                # Higher penalty for more severe defects\n",
        "                if defect['defect_type'] == 'Mold':\n",
        "                    defect_penalty += 0.3\n",
        "                elif defect['defect_type'] == 'Insect_Damage':\n",
        "                    defect_penalty += 0.25\n",
        "                elif defect['defect_type'] == 'Discoloration':\n",
        "                    defect_penalty += 0.15\n",
        "                elif defect['defect_type'] == 'Physical_Damage':\n",
        "                    defect_penalty += 0.1\n",
        "                \n",
        "                # Additional penalty based on defect area\n",
        "                defect_penalty += min(0.2, defect['area'] / 10000)  # Normalize area\n",
        "        \n",
        "        # Calculate final health score\n",
        "        health_score = max(0.0, min(1.0, base_score - defect_penalty))\n",
        "        \n",
        "        return {\n",
        "            'score': health_score,\n",
        "            'percentage': health_score * 100,\n",
        "            'grade': self._get_health_grade(health_score),\n",
        "            'defect_count': len(defects) if defects else 0\n",
        "        }\n",
        "    \n",
        "    def _get_health_grade(self, score):\n",
        "        \"\"\"Convert health score to letter grade\"\"\"\n",
        "        if score >= 0.9:\n",
        "            return 'A+'\n",
        "        elif score >= 0.8:\n",
        "            return 'A'\n",
        "        elif score >= 0.7:\n",
        "            return 'B+'\n",
        "        elif score >= 0.6:\n",
        "            return 'B'\n",
        "        elif score >= 0.5:\n",
        "            return 'C+'\n",
        "        elif score >= 0.4:\n",
        "            return 'C'\n",
        "        elif score >= 0.3:\n",
        "            return 'D'\n",
        "        else:\n",
        "            return 'F'\n",
        "\n",
        "# Utility functions\n",
        "\n",
        "def create_models(device: str = 'cpu'):\n",
        "    \"\"\"Create and initialize all models\"\"\"\n",
        "    device = torch.device(device)\n",
        "    \n",
        "    # Initialize models\n",
        "    cnn = BeanClassifierCNN(num_classes=4, pretrained=True)\n",
        "    defect_detector = DefectDetectorMaskRCNN(num_classes=4, pretrained=True)\n",
        "    lstm = ShelfLifeLSTM(input_size=64, hidden_size=128, num_layers=2)\n",
        "    \n",
        "    # Move to device\n",
        "    cnn.to(device)\n",
        "    defect_detector.to(device)\n",
        "    lstm.to(device)\n",
        "    \n",
        "    # Create ensemble\n",
        "    ensemble = BeanScanEnsemble(cnn, defect_detector, lstm)\n",
        "    ensemble.to(device)\n",
        "    \n",
        "    return {\n",
        "        'cnn': cnn,\n",
        "        'defect_detector': defect_detector,\n",
        "        'lstm': lstm,\n",
        "        'ensemble': ensemble\n",
        "    }\n",
        "\n",
        "\n",
        "def save_models(models: Dict, save_dir: str = './models'):\n",
        "    \"\"\"Save all models\"\"\"\n",
        "    import os\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    \n",
        "    for name, model in models.items():\n",
        "        torch.save(model.state_dict(), os.path.join(save_dir, f'{name}.pth'))\n",
        "        print(f\"âœ… Saved {name} model\")\n",
        "\n",
        "\n",
        "def load_models(device: str = 'cpu', model_dir: str = './models'):\n",
        "    \"\"\"Load all models\"\"\"\n",
        "    device = torch.device(device)\n",
        "    \n",
        "    # Create models\n",
        "    models = create_models(device)\n",
        "    \n",
        "    # Load saved weights if available\n",
        "    for name, model in models.items():\n",
        "        model_path = os.path.join(model_dir, f'{name}.pth')\n",
        "        if os.path.exists(model_path):\n",
        "            model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "            print(f\"âœ… Loaded {name} model from {model_path}\")\n",
        "        else:\n",
        "            print(f\"âš ï¸  No saved weights found for {name}, using initialized weights\")\n",
        "    \n",
        "    return models\n",
        "'''\n",
        "\n",
        "train_models_src = r'''import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "import json\n",
        "from typing import Dict, List, Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "from custom_models import (\n",
        "    BeanClassifierCNN, \n",
        "    DefectDetectorMaskRCNN, \n",
        "    ShelfLifeLSTM,\n",
        "    BeanScanEnsemble,\n",
        "    create_models\n",
        ")\n",
        "\n",
        "class BeanImageDataset(Dataset):\n",
        "    \"\"\"Dataset for bean images with labels\"\"\"\n",
        "    \n",
        "    def __init__(self, data_dir: str, transform=None, split: str = 'train'):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.split = split\n",
        "        \n",
        "        # Load annotations\n",
        "        self.annotations = self._load_annotations()\n",
        "        \n",
        "        # Image transformations\n",
        "        if self.transform is None:\n",
        "            self.transform = transforms.Compose([\n",
        "                transforms.Resize((224, 224)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
        "                                   std=[0.229, 0.224, 0.225])\n",
        "            ])\n",
        "    \n",
        "    def _load_annotations(self):\n",
        "        \"\"\"Load dataset annotations\"\"\"\n",
        "        annotations_file = os.path.join(self.data_dir, f'{self.split}_annotations.json')\n",
        "        if os.path.exists(annotations_file):\n",
        "            with open(annotations_file, 'r') as f:\n",
        "                return json.load(f)\n",
        "        else:\n",
        "            # Create dummy annotations for testing\n",
        "            return self._create_dummy_annotations()\n",
        "    \n",
        "    def _create_dummy_annotations(self):\n",
        "        \"\"\"Create dummy annotations for testing\"\"\"\n",
        "        annotations = []\n",
        "        bean_types = [\"Arabica\", \"Robusta\", \"Liberica\", \"Excelsa\"]\n",
        "        \n",
        "        # Create dummy data\n",
        "        for i in range(100):\n",
        "            annotation = {\n",
        "                'image_id': f'bean_{i:04d}.jpg',\n",
        "                'bean_type': bean_types[i % len(bean_types)],\n",
        "                'defects': [\n",
        "                    {\n",
        "                        'type': 'Mold' if i % 10 == 0 else 'None',\n",
        "                        'bbox': [10, 10, 100, 100] if i % 10 == 0 else [0, 0, 0, 0],\n",
        "                        'mask': np.zeros((224, 224)).tolist() if i % 10 == 0 else []\n",
        "                    }\n",
        "                ],\n",
        "                'health_score': max(0.1, 1.0 - (i % 10) * 0.1)\n",
        "            }\n",
        "            annotations.append(annotation)\n",
        "        \n",
        "        return annotations\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        annotation = self.annotations[idx]\n",
        "        \n",
        "        # Load image (create dummy if not exists)\n",
        "        image_path = os.path.join(self.data_dir, annotation['image_id'])\n",
        "        if os.path.exists(image_path):\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "        else:\n",
        "            # Create dummy image\n",
        "            image = Image.new('RGB', (224, 224), color=(139, 69, 19))  # Brown color\n",
        "        \n",
        "        # Apply transformations\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        \n",
        "        # Prepare labels\n",
        "        bean_type_label = self._get_bean_type_label(annotation['bean_type'])\n",
        "        defect_labels = self._get_defect_labels(annotation['defects'])\n",
        "        \n",
        "        return {\n",
        "            'image': image,\n",
        "            'bean_type_label': bean_type_label,\n",
        "            'defect_labels': defect_labels,\n",
        "            'health_score': annotation['health_score'],\n",
        "            'image_id': annotation['image_id']\n",
        "        }\n",
        "    \n",
        "    def _get_bean_type_label(self, bean_type: str):\n",
        "        \"\"\"Convert bean type to label index\"\"\"\n",
        "        bean_types = [\"Arabica\", \"Robusta\", \"Liberica\", \"Excelsa\"]\n",
        "        return bean_types.index(bean_type) if bean_type in bean_types else 0\n",
        "    \n",
        "    def _get_defect_labels(self, defects: List):\n",
        "        \"\"\"Convert defects to label format\"\"\"\n",
        "        defect_types = [\"Mold\", \"Insect_Damage\", \"Discoloration\", \"Physical_Damage\"]\n",
        "        labels = []\n",
        "        \n",
        "        for defect in defects:\n",
        "            if defect['type'] in defect_types:\n",
        "                label = {\n",
        "                    'boxes': torch.tensor([defect['bbox']], dtype=torch.float32),\n",
        "                    'labels': torch.tensor([defect_types.index(defect['type']) + 1], dtype=torch.long),\n",
        "                    'masks': torch.tensor([defect['mask']], dtype=torch.uint8)\n",
        "                }\n",
        "                labels.append(label)\n",
        "        \n",
        "        return labels\n",
        "\n",
        "class ModelTrainer:\n",
        "    \"\"\"Trainer class for all models\"\"\"\n",
        "    \n",
        "    def __init__(self, device: str = 'cpu', models_dir: str = './models'):\n",
        "        self.device = torch.device(device)\n",
        "        self.models_dir = models_dir\n",
        "        os.makedirs(models_dir, exist_ok=True)\n",
        "        \n",
        "        # Initialize models\n",
        "        self.models = create_models(device)\n",
        "        \n",
        "        # Training parameters\n",
        "        self.learning_rate = 0.001\n",
        "        self.batch_size = 16\n",
        "        self.num_epochs = 50\n",
        "        self.save_interval = 10\n",
        "        \n",
        "        # Loss functions\n",
        "        self.cnn_criterion = nn.CrossEntropyLoss()\n",
        "        self.defect_criterion = self._get_defect_loss()\n",
        "        \n",
        "        # Optimizers\n",
        "        self.optimizers = self._create_optimizers()\n",
        "        \n",
        "        # Training history\n",
        "        self.training_history = {\n",
        "            'cnn_loss': [], 'cnn_acc': [],\n",
        "            'defect_loss': [], 'defect_map': [],\n",
        "            'lstm_loss': [], 'lstm_mae': []\n",
        "        }\n",
        "    \n",
        "    def _get_defect_loss(self):\n",
        "        \"\"\"Get loss function for defect detection\"\"\"\n",
        "        return {\n",
        "            'classification': nn.CrossEntropyLoss(),\n",
        "            'bbox_regression': nn.SmoothL1Loss(),\n",
        "            'mask_loss': nn.BCEWithLogitsLoss()\n",
        "        }\n",
        "    \n",
        "    def _create_optimizers(self):\n",
        "        \"\"\"Create optimizers for all models\"\"\"\n",
        "        return {\n",
        "            'cnn': optim.Adam(self.models['cnn'].parameters(), lr=self.learning_rate),\n",
        "            'defect_detector': optim.Adam(self.models['defect_detector'].parameters(), lr=self.learning_rate),\n",
        "            'lstm': optim.Adam(self.models['lstm'].parameters(), lr=self.learning_rate)\n",
        "        }\n",
        "    \n",
        "    def train_cnn(self, train_loader: DataLoader, val_loader: DataLoader = None):\n",
        "        \"\"\"Train the CNN classifier\"\"\"\n",
        "        print(\"ðŸš€ Training CNN Classifier...\")\n",
        "        \n",
        "        model = self.models['cnn']\n",
        "        optimizer = self.optimizers['cnn']\n",
        "        model.train()\n",
        "        \n",
        "        for epoch in range(self.num_epochs):\n",
        "            running_loss = 0.0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            \n",
        "            # Training loop\n",
        "            for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{self.num_epochs}'):\n",
        "                images = batch['image'].to(self.device)\n",
        "                labels = batch['bean_type_label'].to(self.device)\n",
        "                \n",
        "                # Forward pass\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(images)\n",
        "                loss = self.cnn_criterion(outputs, labels)\n",
        "                \n",
        "                # Backward pass\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                \n",
        "                # Statistics\n",
        "                running_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "            \n",
        "            # Calculate epoch metrics\n",
        "            epoch_loss = running_loss / len(train_loader)\n",
        "            epoch_acc = 100 * correct / total\n",
        "            \n",
        "            self.training_history['cnn_loss'].append(epoch_loss)\n",
        "            self.training_history['cnn_acc'].append(epoch_acc)\n",
        "            \n",
        "            print(f'Epoch {epoch+1}: Loss={epoch_loss:.4f}, Accuracy={epoch_acc:.2f}%')\n",
        "            \n",
        "            # Save model periodically\n",
        "            if (epoch + 1) % self.save_interval == 0:\n",
        "                self._save_model('cnn', epoch + 1)\n",
        "        \n",
        "        print(\"âœ… CNN Training Complete!\")\n",
        "        return self.training_history['cnn_loss'], self.training_history['cnn_acc']\n",
        "    \n",
        "    def train_defect_detector(self, train_loader: DataLoader, val_loader: DataLoader = None):\n",
        "        \"\"\"Train the Mask R-CNN defect detector\"\"\"\n",
        "        print(\"ðŸš€ Training Defect Detector...\")\n",
        "        \n",
        "        model = self.models['defect_detector']\n",
        "        optimizer = self.optimizers['defect_detector']\n",
        "        model.train()\n",
        "        \n",
        "        for epoch in range(self.num_epochs):\n",
        "            running_loss = 0.0\n",
        "            \n",
        "            # Training loop\n",
        "            for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{self.num_epochs}'):\n",
        "                images = batch['image'].to(self.device)\n",
        "                targets = batch['defect_labels']\n",
        "                \n",
        "                # Prepare targets for Mask R-CNN\n",
        "                formatted_targets = self._format_targets(targets)\n",
        "                \n",
        "                # Forward pass\n",
        "                optimizer.zero_grad()\n",
        "                loss_dict = model(images, formatted_targets)\n",
        "                \n",
        "                # Calculate total loss\n",
        "                total_loss = sum(loss_dict.values())\n",
        "                \n",
        "                # Backward pass\n",
        "                total_loss.backward()\n",
        "                optimizer.step()\n",
        "                \n",
        "                # Statistics\n",
        "                running_loss += total_loss.item()\n",
        "            \n",
        "            # Calculate epoch metrics\n",
        "            epoch_loss = running_loss / len(train_loader)\n",
        "            self.training_history['defect_loss'].append(epoch_loss)\n",
        "            \n",
        "            print(f'Epoch {epoch+1}: Loss={epoch_loss:.4f}')\n",
        "            \n",
        "            # Save model periodically\n",
        "            if (epoch + 1) % self.save_interval == 0:\n",
        "                self._save_model('defect_detector', epoch + 1)\n",
        "        \n",
        "        print(\"âœ… Defect Detector Training Complete!\")\n",
        "        return self.training_history['defect_loss']\n",
        "    \n",
        "    def train_lstm(self, train_loader: DataLoader, val_loader: DataLoader = None):\n",
        "        \"\"\"Train the LSTM for shelf life prediction\"\"\"\n",
        "        print(\"ðŸš€ Training LSTM...\")\n",
        "        \n",
        "        model = self.models['lstm']\n",
        "        optimizer = self.optimizers['lstm']\n",
        "        model.train()\n",
        "        \n",
        "        for epoch in range(self.num_epochs):\n",
        "            running_loss = 0.0\n",
        "            running_mae = 0.0\n",
        "            \n",
        "            # Training loop\n",
        "            for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{self.num_epochs}'):\n",
        "                # Create dummy sequence data for training\n",
        "                seq_length = 10\n",
        "                batch_size = batch['image'].size(0)\n",
        "                \n",
        "                # Generate dummy defect sequences\n",
        "                sequences = torch.randn(batch_size, seq_length, 64).to(self.device)\n",
        "                targets = torch.tensor([batch['health_score'] * 30 for _ in range(batch_size)], \n",
        "                                     dtype=torch.float32).to(self.device)  # Convert to days\n",
        "                \n",
        "                # Forward pass\n",
        "                optimizer.zero_grad()\n",
        "                outputs, _ = model(sequences)\n",
        "                loss = nn.MSELoss()(outputs.squeeze(), targets)\n",
        "                \n",
        "                # Backward pass\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                \n",
        "                # Statistics\n",
        "                running_loss += loss.item()\n",
        "                running_mae += torch.mean(torch.abs(outputs.squeeze() - targets)).item()\n",
        "            \n",
        "            # Calculate epoch metrics\n",
        "            epoch_loss = running_loss / len(train_loader)\n",
        "            epoch_mae = running_mae / len(train_loader)\n",
        "            \n",
        "            self.training_history['lstm_loss'].append(epoch_loss)\n",
        "            self.training_history['lstm_mae'].append(epoch_mae)\n",
        "            \n",
        "            print(f'Epoch {epoch+1}: Loss={epoch_loss:.4f}, MAE={epoch_mae:.2f}')\n",
        "            \n",
        "            # Save model periodically\n",
        "            if (epoch + 1) % self.save_interval == 0:\n",
        "                self._save_model('lstm', epoch + 1)\n",
        "        \n",
        "        print(\"âœ… LSTM Training Complete!\")\n",
        "        return self.training_history['lstm_loss'], self.training_history['lstm_mae']\n",
        "    \n",
        "    def _format_targets(self, targets: List):\n",
        "        \"\"\"Format targets for Mask R-CNN training\"\"\"\n",
        "        formatted = []\n",
        "        for target in targets:\n",
        "            if target:  # If defects exist\n",
        "                formatted.append({\n",
        "                    'boxes': target[0]['boxes'].to(self.device),\n",
        "                    'labels': target[0]['labels'].to(self.device),\n",
        "                    'masks': target[0]['masks'].to(self.device)\n",
        "                })\n",
        "            else:  # No defects\n",
        "                formatted.append({\n",
        "                    'boxes': torch.empty((0, 4), dtype=torch.float32).to(self.device),\n",
        "                    'labels': torch.empty((0,), dtype=torch.long).to(self.device),\n",
        "                    'masks': torch.empty((0, 224, 224), dtype=torch.uint8).to(self.device)\n",
        "                })\n",
        "        return formatted\n",
        "    \n",
        "    def _save_model(self, model_name: str, epoch: int):\n",
        "        \"\"\"Save model checkpoint\"\"\"\n",
        "        save_path = os.path.join(self.models_dir, f'{model_name}_epoch_{epoch}.pth')\n",
        "        torch.save(self.models[model_name].state_dict(), save_path)\n",
        "        print(f\"ðŸ’¾ Saved {model_name} checkpoint: {save_path}\")\n",
        "    \n",
        "    def save_final_models(self):\n",
        "        \"\"\"Save final trained models\"\"\"\n",
        "        for name, model in self.models.items():\n",
        "            save_path = os.path.join(self.models_dir, f'{name}_final.pth')\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            print(f\"ðŸ’¾ Saved final {name} model: {save_path}\")\n",
        "    \n",
        "    def plot_training_history(self):\n",
        "        \"\"\"Plot training history\"\"\"\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "        \n",
        "        # CNN metrics\n",
        "        axes[0, 0].plot(self.training_history['cnn_loss'])\n",
        "        axes[0, 0].set_title('CNN Loss')\n",
        "        axes[0, 0].set_xlabel('Epoch')\n",
        "        axes[0, 0].set_ylabel('Loss')\n",
        "        \n",
        "        axes[0, 1].plot(self.training_history['cnn_acc'])\n",
        "        axes[0, 1].set_title('CNN Accuracy')\n",
        "        axes[0, 1].set_xlabel('Epoch')\n",
        "        axes[0, 1].set_ylabel('Accuracy (%)')\n",
        "        \n",
        "        # Defect detector metrics\n",
        "        axes[0, 2].plot(self.training_history['defect_loss'])\n",
        "        axes[0, 2].set_title('Defect Detector Loss')\n",
        "        axes[0, 2].set_xlabel('Epoch')\n",
        "        axes[0, 2].set_ylabel('Loss')\n",
        "        \n",
        "        # LSTM metrics\n",
        "        axes[1, 0].plot(self.training_history['lstm_loss'])\n",
        "        axes[1, 0].set_title('LSTM Loss')\n",
        "        axes[1, 0].set_xlabel('Epoch')\n",
        "        axes[1, 0].set_ylabel('Loss')\n",
        "        \n",
        "        axes[1, 1].plot(self.training_history['lstm_mae'])\n",
        "        axes[1, 1].set_title('LSTM MAE')\n",
        "        axes[1, 1].set_xlabel('Epoch')\n",
        "        axes[1, 1].set_ylabel('MAE')\n",
        "        \n",
        "        # Hide empty subplot\n",
        "        axes[1, 2].set_visible(False)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.models_dir, 'training_history.png'))\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main training function\"\"\"\n",
        "    print(\"ðŸŽ¯ BeanScan Deep Learning Model Training\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Initialize trainer\n",
        "    trainer = ModelTrainer(device='cpu')  # Use 'cuda' if GPU available\n",
        "    \n",
        "    # Create dummy datasets (replace with real data)\n",
        "    train_dataset = BeanImageDataset('./data', split='train')\n",
        "    val_dataset = BeanImageDataset('./data', split='val')\n",
        "    \n",
        "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "    \n",
        "    print(f\"ðŸ“Š Dataset sizes: Train={len(train_dataset)}, Val={len(val_dataset)}\")\n",
        "    \n",
        "    # Train all models\n",
        "    try:\n",
        "        # Train CNN\n",
        "        cnn_loss, cnn_acc = trainer.train_cnn(train_loader, val_loader)\n",
        "        \n",
        "        # Train Defect Detector\n",
        "        defect_loss = trainer.train_defect_detector(train_loader, val_loader)\n",
        "        \n",
        "        # Train LSTM\n",
        "        lstm_loss, lstm_mae = trainer.train_lstm(train_loader, val_loader)\n",
        "        \n",
        "        # Save final models\n",
        "        trainer.save_final_models()\n",
        "        \n",
        "        # Plot training history\n",
        "        trainer.plot_training_history()\n",
        "        \n",
        "        print(\"\\nðŸŽ‰ All models trained successfully!\")\n",
        "        print(\"ðŸ“ Models saved in:\", trainer.models_dir)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Training failed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "'''\n",
        "\n",
        "with open(os.path.join(base_dir, 'custom_models.py'), 'w', encoding='utf-8') as f:\n",
        "    f.write(custom_models_src)\n",
        "with open(os.path.join(base_dir, 'train_models.py'), 'w', encoding='utf-8') as f:\n",
        "    f.write(train_models_src)\n",
        "\n",
        "import sys\n",
        "if '/kaggle/working/backend' not in sys.path:\n",
        "    sys.path.append('/kaggle/working/backend')\n",
        "if '/kaggle/working/backend/ml' not in sys.path:\n",
        "    sys.path.append('/kaggle/working/backend/ml')\n",
        "\n",
        "print('Wrote inline modules to', base_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Write code files inline so we don't need a code dataset ===\n",
        "import os\n",
        "base_dir = \"/kaggle/working/backend/ml\"\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "custom_models_src = r'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import mobilenet_v3_small, mobilenet_v3_large\n",
        "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
        "from torchvision.models.detection import fasterrcnn_mobilenet_v3_large_fpn\n",
        "from torchvision.models.detection.backbone_utils import BackboneWithFPN\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "import torchvision.transforms as transforms\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import numpy as np\n",
        "\n",
        "class MobileNetV3Backbone(nn.Module):\n",
        "    \"\"\"Custom MobileNetV3 backbone for feature extraction\"\"\"\n",
        "    \n",
        "    def __init__(self, pretrained: bool = True, width_mult: float = 1.0):\n",
        "        super().__init__()\n",
        "        # Load pretrained MobileNetV3\n",
        "        if pretrained:\n",
        "            self.backbone = mobilenet_v3_small(pretrained=True)\n",
        "        else:\n",
        "            self.backbone = mobilenet_v3_small(pretrained=False)\n",
        "        \n",
        "        # Extract features from different layers\n",
        "        self.features = self.backbone.features\n",
        "        \n",
        "        # Feature dimensions for different scales\n",
        "        self.feature_channels = [16, 24, 40, 48, 96, 576]\n",
        "        \n",
        "    def forward(self, x):\n",
        "        features = []\n",
        "        for i, layer in enumerate(self.features):\n",
        "            x = layer(x)\n",
        "            if i in [2, 4, 6, 8, 10, 12]:  # Key feature layers\n",
        "                features.append(x)\n",
        "        return features\n",
        "\n",
        "class BeanClassifierCNN(nn.Module):\n",
        "    \"\"\"CNN for bean type classification using MobileNetV3 backbone\"\"\"\n",
        "    \n",
        "    def __init__(self, num_classes: int = 4, pretrained: bool = True):\n",
        "        super().__init__()\n",
        "        self.backbone = MobileNetV3Backbone(pretrained=pretrained)\n",
        "        \n",
        "        # Classification head (increased dropout ~0.3 to mitigate overfitting)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(576, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "        \n",
        "        # Bean type names\n",
        "        self.class_names = [\"Arabica\", \"Robusta\", \"Liberica\", \"Excelsa\"]\n",
        "        \n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)\n",
        "        # Use the last feature map for classification\n",
        "        x = features[-1]\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "    \n",
        "    def predict(self, x, threshold: float = 0.5):\n",
        "        \"\"\"Predict bean type with confidence\"\"\"\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            logits = self.forward(x)\n",
        "            probabilities = F.softmax(logits, dim=1)\n",
        "            confidence, predicted = torch.max(probabilities, 1)\n",
        "            \n",
        "            # Filter by confidence threshold\n",
        "            mask = confidence >= threshold\n",
        "            predictions = []\n",
        "            \n",
        "            for i in range(len(predicted)):\n",
        "                if mask[i]:\n",
        "                    predictions.append({\n",
        "                        'class': self.class_names[predicted[i].item()],\n",
        "                        'confidence': confidence[i].item(),\n",
        "                        'probabilities': probabilities[i].tolist()\n",
        "                    })\n",
        "                else:\n",
        "                    predictions.append({\n",
        "                        'class': 'Unknown',\n",
        "                        'confidence': confidence[i].item(),\n",
        "                        'probabilities': probabilities[i].tolist()\n",
        "                    })\n",
        "            \n",
        "            return predictions\n",
        "\n",
        "class DefectDetectorMaskRCNN(nn.Module):\n",
        "    \"\"\"Mask R-CNN for defect detection using MobileNetV3 backbone\"\"\"\n",
        "    \n",
        "    def __init__(self, num_classes: int = 4, pretrained: bool = True):\n",
        "        super().__init__()\n",
        "        # Create custom backbone with MobileNetV3\n",
        "        self.backbone = MobileNetV3Backbone(pretrained=pretrained)\n",
        "        \n",
        "        # Create FPN from backbone features\n",
        "        self.fpn = BackboneWithFPN(\n",
        "            self.backbone,\n",
        "            return_layers={'0': '0', '1': '1', '2': '2', '3': '3', '4': '4', '5': '5'},\n",
        "            in_channels_list=[16, 24, 40, 48, 96, 576],\n",
        "            out_channels=256\n",
        "        )\n",
        "        \n",
        "        # Create Mask R-CNN with custom backbone\n",
        "        self.mask_rcnn = maskrcnn_resnet50_fpn(\n",
        "            pretrained=False,\n",
        "            num_classes=num_classes + 1  # +1 for background\n",
        "        )\n",
        "        \n",
        "        # Replace backbone\n",
        "        self.mask_rcnn.backbone = self.fpn\n",
        "        \n",
        "        # Customize box and mask predictors\n",
        "        in_features = self.mask_rcnn.roi_heads.box_predictor.cls_score.in_features\n",
        "        self.mask_rcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes + 1)\n",
        "        \n",
        "        in_features_mask = self.mask_rcnn.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "        hidden_layer = 256\n",
        "        self.mask_rcnn.roi_heads.mask_predictor = MaskRCNNPredictor(\n",
        "            in_features_mask, hidden_layer, num_classes + 1\n",
        "        )\n",
        "        \n",
        "        # Defect types\n",
        "        self.defect_types = [\"Mold\", \"Insect_Damage\", \"Discoloration\", \"Physical_Damage\"]\n",
        "        \n",
        "    def forward(self, images, targets=None):\n",
        "        return self.mask_rcnn(images, targets)\n",
        "    \n",
        "    def detect_defects(self, image, confidence_threshold: float = 0.5):\n",
        "        \"\"\"Detect defects in bean image\"\"\"\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            # Prepare image\n",
        "            if len(image.shape) == 3:\n",
        "                image = image.unsqueeze(0)\n",
        "            \n",
        "            # Get predictions\n",
        "            predictions = self.forward(image)\n",
        "            \n",
        "            # Process results\n",
        "            defects = []\n",
        "            for pred in predictions:\n",
        "                boxes = pred['boxes']\n",
        "                scores = pred['scores']\n",
        "                masks = pred['masks']\n",
        "                labels = pred['labels']\n",
        "                \n",
        "                for i in range(len(scores)):\n",
        "                    if scores[i] >= confidence_threshold:\n",
        "                        defect = {\n",
        "                            'bbox': boxes[i].tolist(),\n",
        "                            'confidence': scores[i].item(),\n",
        "                            'mask': masks[i].squeeze().tolist(),\n",
        "                            'defect_type': self.defect_types[labels[i].item() - 1],  # -1 for background\n",
        "                            'area': torch.sum(masks[i]).item(),\n",
        "                            'coordinates': {\n",
        "                                'x1': boxes[i][0].item(),\n",
        "                                'y1': boxes[i][1].item(),\n",
        "                                'x2': boxes[i][2].item(),\n",
        "                                'y2': boxes[i][3].item()\n",
        "                            }\n",
        "                        }\n",
        "                        defects.append(defect)\n",
        "            \n",
        "            return defects\n",
        "\n",
        "class DefectDetectorFasterRCNN(nn.Module):\n",
        "    \"\"\"Faster R-CNN detector (bounding boxes only) for bean defects\"\"\"\n",
        "    \n",
        "    def __init__(self, num_classes: int = 7, pretrained: bool = True,\n",
        "                 class_names: Optional[List[str]] = None):\n",
        "        super().__init__()\n",
        "        # num_classes should include background (>=2)\n",
        "        self.num_classes = max(2, num_classes)\n",
        "        self.model = fasterrcnn_mobilenet_v3_large_fpn(pretrained=pretrained)\n",
        "        in_features = self.model.roi_heads.box_predictor.cls_score.in_features\n",
        "        self.model.roi_heads.box_predictor = FastRCNNPredictor(in_features, self.num_classes)\n",
        "        \n",
        "        default_classes = [\n",
        "            \"insect_damage\",\n",
        "            \"nugget\",\n",
        "            \"quaker\",\n",
        "            \"roasted-beans\",\n",
        "            \"shell\",\n",
        "            \"under_roast\"\n",
        "        ]\n",
        "        self.class_names = [\"__background__\"] + (class_names or default_classes)\n",
        "    \n",
        "    def forward(self, images, targets=None):\n",
        "        return self.model(images, targets)\n",
        "    \n",
        "    def detect(self, image, confidence_threshold: float = 0.5):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            if len(image.shape) == 3:\n",
        "                image = image.unsqueeze(0)\n",
        "            outputs = self.forward(image)\n",
        "            detections = []\n",
        "            for pred in outputs:\n",
        "                boxes = pred['boxes']\n",
        "                scores = pred['scores']\n",
        "                labels = pred['labels']\n",
        "                for i in range(len(scores)):\n",
        "                    if scores[i] >= confidence_threshold:\n",
        "                        detections.append({\n",
        "                            'bbox': boxes[i].tolist(),\n",
        "                            'score': scores[i].item(),\n",
        "                            'label': self.class_names[labels[i].item()]\n",
        "                        })\n",
        "            return detections\n",
        "\n",
        "class ShelfLifeLSTM(nn.Module):\n",
        "    \"\"\"LSTM for shelf life prediction based on defect progression\"\"\"\n",
        "    \n",
        "    def __init__(self, input_size: int = 64, hidden_size: int = 128, num_layers: int = 2, dropout: float = 0.2):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        # LSTM layers\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        \n",
        "        # Attention mechanism\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            embed_dim=hidden_size * 2,  # *2 for bidirectional\n",
        "            num_heads=8,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        \n",
        "        # Prediction head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_size * 2, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size, hidden_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size // 2, 1)  # Predict days until expiration\n",
        "        )\n",
        "        \n",
        "        # Shelf life categories\n",
        "        self.shelf_life_categories = [\"Expired\", \"Critical\", \"Warning\", \"Good\", \"Excellent\"]\n",
        "        \n",
        "    def forward(self, x, hidden=None):\n",
        "        # x shape: (batch_size, seq_len, input_size)\n",
        "        batch_size = x.size(0)\n",
        "        \n",
        "        # Initialize hidden state if not provided\n",
        "        if hidden is None:\n",
        "            h0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size).to(x.device)\n",
        "            c0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size).to(x.device)\n",
        "            hidden = (h0, c0)\n",
        "        \n",
        "        # LSTM forward pass\n",
        "        lstm_out, hidden = self.lstm(x, hidden)\n",
        "        \n",
        "        # Apply attention\n",
        "        lstm_out = lstm_out.transpose(0, 1)  # (seq_len, batch_size, hidden_size*2)\n",
        "        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
        "        attn_out = attn_out.transpose(0, 1)  # (batch_size, seq_len, hidden_size*2)\n",
        "        \n",
        "        # Global average pooling\n",
        "        pooled = torch.mean(attn_out, dim=1)  # (batch_size, hidden_size*2)\n",
        "        \n",
        "        # Predict shelf life\n",
        "        shelf_life = self.classifier(pooled)\n",
        "        \n",
        "        return shelf_life, hidden\n",
        "    \n",
        "    def predict_shelf_life(self, defect_sequence, confidence_threshold: float = 0.7):\n",
        "        \"\"\"Predict shelf life based on defect progression sequence\"\"\"\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            # Prepare input sequence\n",
        "            if isinstance(defect_sequence, list):\n",
        "                defect_sequence = torch.tensor(defect_sequence, dtype=torch.float32)\n",
        "            \n",
        "            if len(defect_sequence.shape) == 2:\n",
        "                defect_sequence = defect_sequence.unsqueeze(0)  # Add batch dimension\n",
        "            \n",
        "            # Get prediction\n",
        "            shelf_life_days, _ = self.forward(defect_sequence)\n",
        "            predicted_days = shelf_life_days.item()\n",
        "            \n",
        "            # Categorize shelf life\n",
        "            if predicted_days <= 0:\n",
        "                category = \"Expired\"\n",
        "                confidence = 1.0\n",
        "            elif predicted_days <= 3:\n",
        "                category = \"Critical\"\n",
        "                confidence = 0.9\n",
        "            elif predicted_days <= 7:\n",
        "                category = \"Warning\"\n",
        "                confidence = 0.8\n",
        "            elif predicted_days <= 14:\n",
        "                category = \"Good\"\n",
        "                confidence = 0.7\n",
        "            else:\n",
        "                category = \"Excellent\"\n",
        "                confidence = 0.6\n",
        "            \n",
        "            # Adjust confidence based on threshold\n",
        "            if confidence < confidence_threshold:\n",
        "                category = \"Uncertain\"\n",
        "            \n",
        "            return {\n",
        "                'predicted_days': max(0, int(predicted_days)),\n",
        "                'category': category,\n",
        "                'confidence': confidence,\n",
        "                'raw_prediction': predicted_days\n",
        "            }\n",
        "\n",
        "class BeanScanEnsemble(nn.Module):\n",
        "    \"\"\"Ensemble model combining CNN, Mask R-CNN, and LSTM\"\"\"\n",
        "    \n",
        "    def __init__(self, cnn_model: BeanClassifierCNN, \n",
        "                 defect_model: DefectDetectorMaskRCNN,\n",
        "                 lstm_model: ShelfLifeLSTM):\n",
        "        super().__init__()\n",
        "        self.cnn_model = cnn_model\n",
        "        self.defect_model = defect_model\n",
        "        self.lstm_model = lstm_model\n",
        "        \n",
        "    def forward(self, image, defect_sequence=None):\n",
        "        \"\"\"Complete bean analysis pipeline\"\"\"\n",
        "        results = {}\n",
        "        \n",
        "        # 1. Bean type classification\n",
        "        bean_type = self.cnn_model.predict(image)\n",
        "        results['bean_classification'] = bean_type\n",
        "        \n",
        "        # 2. Defect detection\n",
        "        defects = self.defect_model.detect_defects(image)\n",
        "        results['defect_detection'] = defects\n",
        "        \n",
        "        # 3. Shelf life prediction (if sequence provided)\n",
        "        if defect_sequence is not None:\n",
        "            shelf_life = self.lstm_model.predict_shelf_life(defect_sequence)\n",
        "            results['shelf_life_prediction'] = shelf_life\n",
        "        \n",
        "        # 4. Calculate overall health score\n",
        "        health_score = self._calculate_health_score(bean_type, defects)\n",
        "        results['health_score'] = health_score\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def _calculate_health_score(self, bean_type, defects):\n",
        "        \"\"\"Calculate overall bean health score\"\"\"\n",
        "        # Base score from bean type confidence\n",
        "        base_score = bean_type[0]['confidence'] if bean_type else 0.5\n",
        "        \n",
        "        # Penalty for defects\n",
        "        defect_penalty = 0\n",
        "        if defects:\n",
        "            for defect in defects:\n",
        "                # Higher penalty for more severe defects\n",
        "                if defect['defect_type'] == 'Mold':\n",
        "                    defect_penalty += 0.3\n",
        "                elif defect['defect_type'] == 'Insect_Damage':\n",
        "                    defect_penalty += 0.25\n",
        "                elif defect['defect_type'] == 'Discoloration':\n",
        "                    defect_penalty += 0.15\n",
        "                elif defect['defect_type'] == 'Physical_Damage':\n",
        "                    defect_penalty += 0.1\n",
        "                \n",
        "                # Additional penalty based on defect area\n",
        "                defect_penalty += min(0.2, defect['area'] / 10000)  # Normalize area\n",
        "        \n",
        "        # Calculate final health score\n",
        "        health_score = max(0.0, min(1.0, base_score - defect_penalty))\n",
        "        \n",
        "        return {\n",
        "            'score': health_score,\n",
        "            'percentage': health_score * 100,\n",
        "            'grade': self._get_health_grade(health_score),\n",
        "            'defect_count': len(defects) if defects else 0\n",
        "        }\n",
        "    \n",
        "    def _get_health_grade(self, score):\n",
        "        \"\"\"Convert health score to letter grade\"\"\"\n",
        "        if score >= 0.9:\n",
        "            return 'A+'\n",
        "        elif score >= 0.8:\n",
        "            return 'A'\n",
        "        elif score >= 0.7:\n",
        "            return 'B+'\n",
        "        elif score >= 0.6:\n",
        "            return 'B'\n",
        "        elif score >= 0.5:\n",
        "            return 'C+'\n",
        "        elif score >= 0.4:\n",
        "            return 'C'\n",
        "        elif score >= 0.3:\n",
        "            return 'D'\n",
        "        else:\n",
        "            return 'F'\n",
        "\n",
        "# Utility functions\n",
        "\n",
        "def create_models(device: str = 'cpu'):\n",
        "    \"\"\"Create and initialize all models\"\"\"\n",
        "    device = torch.device(device)\n",
        "    \n",
        "    # Initialize models\n",
        "    cnn = BeanClassifierCNN(num_classes=4, pretrained=True)\n",
        "    defect_detector = DefectDetectorMaskRCNN(num_classes=4, pretrained=True)\n",
        "    lstm = ShelfLifeLSTM(input_size=64, hidden_size=128, num_layers=2)\n",
        "    \n",
        "    # Move to device\n",
        "    cnn.to(device)\n",
        "    defect_detector.to(device)\n",
        "    lstm.to(device)\n",
        "    \n",
        "    # Create ensemble\n",
        "    ensemble = BeanScanEnsemble(cnn, defect_detector, lstm)\n",
        "    ensemble.to(device)\n",
        "    \n",
        "    return {\n",
        "        'cnn': cnn,\n",
        "        'defect_detector': defect_detector,\n",
        "        'lstm': lstm,\n",
        "        'ensemble': ensemble\n",
        "    }\n",
        "\n",
        "\n",
        "def save_models(models: Dict, save_dir: str = './models'):\n",
        "    \"\"\"Save all models\"\"\"\n",
        "    import os\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    \n",
        "    for name, model in models.items():\n",
        "        torch.save(model.state_dict(), os.path.join(save_dir, f'{name}.pth'))\n",
        "        print(f\"âœ… Saved {name} model\")\n",
        "\n",
        "\n",
        "def load_models(device: str = 'cpu', model_dir: str = './models'):\n",
        "    \"\"\"Load all models\"\"\"\n",
        "    device = torch.device(device)\n",
        "    \n",
        "    # Create models\n",
        "    models = create_models(device)\n",
        "    \n",
        "    # Load saved weights if available\n",
        "    for name, model in models.items():\n",
        "        model_path = os.path.join(model_dir, f'{name}.pth')\n",
        "        if os.path.exists(model_path):\n",
        "            model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "            print(f\"âœ… Loaded {name} model from {model_path}\")\n",
        "        else:\n",
        "            print(f\"âš ï¸  No saved weights found for {name}, using initialized weights\")\n",
        "    \n",
        "    return models\n",
        "'''\n",
        "\n",
        "train_models_src = r''' \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "import json\n",
        "from typing import Dict, List, Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "from custom_models import (\n",
        "    BeanClassifierCNN, \n",
        "    DefectDetectorMaskRCNN, \n",
        "    ShelfLifeLSTM,\n",
        "    BeanScanEnsemble,\n",
        "    create_models\n",
        ")\n",
        "\n",
        "class BeanImageDataset(Dataset):\n",
        "    \"\"\"Dataset for bean images with labels\"\"\"\n",
        "    \n",
        "    def __init__(self, data_dir: str, transform=None, split: str = 'train'):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.split = split\n",
        "        \n",
        "        # Load annotations\n",
        "        self.annotations = self._load_annotations()\n",
        "        \n",
        "        # Image transformations\n",
        "        if self.transform is None:\n",
        "            self.transform = transforms.Compose([\n",
        "                transforms.Resize((224, 224)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
        "                                   std=[0.229, 0.224, 0.225])\n",
        "            ])\n",
        "    \n",
        "    def _load_annotations(self):\n",
        "        \"\"\"Load dataset annotations\"\"\"\n",
        "        annotations_file = os.path.join(self.data_dir, f'{self.split}_annotations.json')\n",
        "        if os.path.exists(annotations_file):\n",
        "            with open(annotations_file, 'r') as f:\n",
        "                return json.load(f)\n",
        "        else:\n",
        "            # Create dummy annotations for testing\n",
        "            return self._create_dummy_annotations()\n",
        "    \n",
        "    def _create_dummy_annotations(self):\n",
        "        \"\"\"Create dummy annotations for testing\"\"\"\n",
        "        annotations = []\n",
        "        bean_types = [\"Arabica\", \"Robusta\", \"Liberica\", \"Excelsa\"]\n",
        "        \n",
        "        # Create dummy data\n",
        "        for i in range(100):\n",
        "            annotation = {\n",
        "                'image_id': f'bean_{i:04d}.jpg',\n",
        "                'bean_type': bean_types[i % len(bean_types)],\n",
        "                'defects': [\n",
        "                    {\n",
        "                        'type': 'Mold' if i % 10 == 0 else 'None',\n",
        "                        'bbox': [10, 10, 100, 100] if i % 10 == 0 else [0, 0, 0, 0],\n",
        "                        'mask': np.zeros((224, 224)).tolist() if i % 10 == 0 else []\n",
        "                    }\n",
        "                ],\n",
        "                'health_score': max(0.1, 1.0 - (i % 10) * 0.1)\n",
        "            }\n",
        "            annotations.append(annotation)\n",
        "        \n",
        "        return annotations\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        annotation = self.annotations[idx]\n",
        "        \n",
        "        # Load image (create dummy if not exists)\n",
        "        image_path = os.path.join(self.data_dir, annotation['image_id'])\n",
        "        if os.path.exists(image_path):\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "        else:\n",
        "            # Create dummy image\n",
        "            image = Image.new('RGB', (224, 224), color=(139, 69, 19))  # Brown color\n",
        "        \n",
        "        # Apply transformations\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        \n",
        "        # Prepare labels\n",
        "        bean_type_label = self._get_bean_type_label(annotation['bean_type'])\n",
        "        defect_labels = self._get_defect_labels(annotation['defects'])\n",
        "        \n",
        "        return {\n",
        "            'image': image,\n",
        "            'bean_type_label': bean_type_label,\n",
        "            'defect_labels': defect_labels,\n",
        "            'health_score': annotation['health_score'],\n",
        "            'image_id': annotation['image_id']\n",
        "        }\n",
        "    \n",
        "    def _get_bean_type_label(self, bean_type: str):\n",
        "        \"\"\"Convert bean type to label index\"\"\"\n",
        "        bean_types = [\"Arabica\", \"Robusta\", \"Liberica\", \"Excelsa\"]\n",
        "        return bean_types.index(bean_type) if bean_type in bean_types else 0\n",
        "    \n",
        "    def _get_defect_labels(self, defects: List):\n",
        "        \"\"\"Convert defects to label format\"\"\"\n",
        "        defect_types = [\"Mold\", \"Insect_Damage\", \"Discoloration\", \"Physical_Damage\"]\n",
        "        labels = []\n",
        "        \n",
        "        for defect in defects:\n",
        "            if defect['type'] in defect_types:\n",
        "                label = {\n",
        "                    'boxes': torch.tensor([defect['bbox']], dtype=torch.float32),\n",
        "                    'labels': torch.tensor([defect_types.index(defect['type']) + 1], dtype=torch.long),\n",
        "                    'masks': torch.tensor([defect['mask']], dtype=torch.uint8)\n",
        "                }\n",
        "                labels.append(label)\n",
        "        \n",
        "        return labels\n",
        "\n",
        "class ModelTrainer:\n",
        "    \"\"\"Trainer class for all models\"\"\"\n",
        "    \n",
        "    def __init__(self, device: str = 'cpu', models_dir: str = './models'):\n",
        "        self.device = torch.device(device)\n",
        "        self.models_dir = models_dir\n",
        "        os.makedirs(models_dir, exist_ok=True)\n",
        "        \n",
        "        # Initialize models\n",
        "        self.models = create_models(device)\n",
        "        \n",
        "        # Training parameters\n",
        "        self.learning_rate = 0.001\n",
        "        self.batch_size = 16\n",
        "        self.num_epochs = 50\n",
        "        self.save_interval = 10\n",
        "        \n",
        "        # Loss functions\n",
        "        self.cnn_criterion = nn.CrossEntropyLoss()\n",
        "        self.defect_criterion = self._get_defect_loss()\n",
        "        \n",
        "        # Optimizers\n",
        "        self.optimizers = self._create_optimizers()\n",
        "        \n",
        "        # Training history\n",
        "        self.training_history = {\n",
        "            'cnn_loss': [], 'cnn_acc': [],\n",
        "            'defect_loss': [], 'defect_map': [],\n",
        "            'lstm_loss': [], 'lstm_mae': []\n",
        "        }\n",
        "    \n",
        "    def _get_defect_loss(self):\n",
        "        \"\"\"Get loss function for defect detection\"\"\"\n",
        "        return {\n",
        "            'classification': nn.CrossEntropyLoss(),\n",
        "            'bbox_regression': nn.SmoothL1Loss(),\n",
        "            'mask_loss': nn.BCEWithLogitsLoss()\n",
        "        }\n",
        "    \n",
        "    def _create_optimizers(self):\n",
        "        \"\"\"Create optimizers for all models\"\"\"\n",
        "        return {\n",
        "            'cnn': optim.Adam(self.models['cnn'].parameters(), lr=self.learning_rate),\n",
        "            'defect_detector': optim.Adam(self.models['defect_detector'].parameters(), lr=self.learning_rate),\n",
        "            'lstm': optim.Adam(self.models['lstm'].parameters(), lr=self.learning_rate)\n",
        "        }\n",
        "    \n",
        "    def train_cnn(self, train_loader: DataLoader, val_loader: DataLoader = None):\n",
        "        \"\"\"Train the CNN classifier\"\"\"\n",
        "        print(\"ðŸš€ Training CNN Classifier...\")\n",
        "        \n",
        "        model = self.models['cnn']\n",
        "        optimizer = self.optimizers['cnn']\n",
        "        model.train()\n",
        "        \n",
        "        for epoch in range(self.num_epochs):\n",
        "            running_loss = 0.0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            \n",
        "            # Training loop\n",
        "            for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{self.num_epochs}'):\n",
        "                images = batch['image'].to(self.device)\n",
        "                labels = batch['bean_type_label'].to(self.device)\n",
        "                \n",
        "                # Forward pass\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(images)\n",
        "                loss = self.cnn_criterion(outputs, labels)\n",
        "                \n",
        "                # Backward pass\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                \n",
        "                # Statistics\n",
        "                running_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "            \n",
        "            # Calculate epoch metrics\n",
        "            epoch_loss = running_loss / len(train_loader)\n",
        "            epoch_acc = 100 * correct / total\n",
        "            \n",
        "            self.training_history['cnn_loss'].append(epoch_loss)\n",
        "            self.training_history['cnn_acc'].append(epoch_acc)\n",
        "            \n",
        "            print(f'Epoch {epoch+1}: Loss={epoch_loss:.4f}, Accuracy={epoch_acc:.2f}%')\n",
        "            \n",
        "            # Save model periodically\n",
        "            if (epoch + 1) % self.save_interval == 0:\n",
        "                self._save_model('cnn', epoch + 1)\n",
        "        \n",
        "        print(\"âœ… CNN Training Complete!\")\n",
        "        return self.training_history['cnn_loss'], self.training_history['cnn_acc']\n",
        "    \n",
        "    def train_defect_detector(self, train_loader: DataLoader, val_loader: DataLoader = None):\n",
        "        \"\"\"Train the Mask R-CNN defect detector\"\"\"\n",
        "        print(\"ðŸš€ Training Defect Detector...\")\n",
        "        \n",
        "        model = self.models['defect_detector']\n",
        "        optimizer = self.optimizers['defect_detector']\n",
        "        model.train()\n",
        "        \n",
        "        for epoch in range(self.num_epochs):\n",
        "            running_loss = 0.0\n",
        "            \n",
        "            # Training loop\n",
        "            for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{self.num_epochs}'):\n",
        "                images = batch['image'].to(self.device)\n",
        "                targets = batch['defect_labels']\n",
        "                \n",
        "                # Prepare targets for Mask R-CNN\n",
        "                formatted_targets = self._format_targets(targets)\n",
        "                \n",
        "                # Forward pass\n",
        "                optimizer.zero_grad()\n",
        "                loss_dict = model(images, formatted_targets)\n",
        "                \n",
        "                # Calculate total loss\n",
        "                total_loss = sum(loss_dict.values())\n",
        "                \n",
        "                # Backward pass\n",
        "                total_loss.backward()\n",
        "                optimizer.step()\n",
        "                \n",
        "                # Statistics\n",
        "                running_loss += total_loss.item()\n",
        "            \n",
        "            # Calculate epoch metrics\n",
        "            epoch_loss = running_loss / len(train_loader)\n",
        "            self.training_history['defect_loss'].append(epoch_loss)\n",
        "            \n",
        "            print(f'Epoch {epoch+1}: Loss={epoch_loss:.4f}')\n",
        "            \n",
        "            # Save model periodically\n",
        "            if (epoch + 1) % self.save_interval == 0:\n",
        "                self._save_model('defect_detector', epoch + 1)\n",
        "        \n",
        "        print(\"âœ… Defect Detector Training Complete!\")\n",
        "        return self.training_history['defect_loss']\n",
        "    \n",
        "    def train_lstm(self, train_loader: DataLoader, val_loader: DataLoader = None):\n",
        "        \"\"\"Train the LSTM for shelf life prediction\"\"\"\n",
        "        print(\"ðŸš€ Training LSTM...\")\n",
        "        \n",
        "        model = self.models['lstm']\n",
        "        optimizer = self.optimizers['lstm']\n",
        "        model.train()\n",
        "        \n",
        "        for epoch in range(self.num_epochs):\n",
        "            running_loss = 0.0\n",
        "            running_mae = 0.0\n",
        "            \n",
        "            # Training loop\n",
        "            for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{self.num_epochs}'):\n",
        "                # Create dummy sequence data for training\n",
        "                seq_length = 10\n",
        "                batch_size = batch['image'].size(0)\n",
        "                \n",
        "                # Generate dummy defect sequences\n",
        "                sequences = torch.randn(batch_size, seq_length, 64).to(self.device)\n",
        "                targets = torch.tensor([batch['health_score'] * 30 for _ in range(batch_size)], \n",
        "                                     dtype=torch.float32).to(self.device)  # Convert to days\n",
        "                \n",
        "                # Forward pass\n",
        "                optimizer.zero_grad()\n",
        "                outputs, _ = model(sequences)\n",
        "                loss = nn.MSELoss()(outputs.squeeze(), targets)\n",
        "                \n",
        "                # Backward pass\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                \n",
        "                # Statistics\n",
        "                running_loss += loss.item()\n",
        "                running_mae += torch.mean(torch.abs(outputs.squeeze() - targets)).item()\n",
        "            \n",
        "            # Calculate epoch metrics\n",
        "            epoch_loss = running_loss / len(train_loader)\n",
        "            epoch_mae = running_mae / len(train_loader)\n",
        "            \n",
        "            self.training_history['lstm_loss'].append(epoch_loss)\n",
        "            self.training_history['lstm_mae'].append(epoch_mae)\n",
        "            \n",
        "            print(f'Epoch {epoch+1}: Loss={epoch_loss:.4f}, MAE={epoch_mae:.2f}')\n",
        "            \n",
        "            # Save model periodically\n",
        "            if (epoch + 1) % self.save_interval == 0:\n",
        "                self._save_model('lstm', epoch + 1)\n",
        "        \n",
        "        print(\"âœ… LSTM Training Complete!\")\n",
        "        return self.training_history['lstm_loss'], self.training_history['lstm_mae']\n",
        "    \n",
        "    def _format_targets(self, targets: List):\n",
        "        \"\"\"Format targets for Mask R-CNN training\"\"\"\n",
        "        formatted = []\n",
        "        for target in targets:\n",
        "            if target:  # If defects exist\n",
        "                formatted.append({\n",
        "                    'boxes': target[0]['boxes'].to(self.device),\n",
        "                    'labels': target[0]['labels'].to(self.device),\n",
        "                    'masks': target[0]['masks'].to(self.device)\n",
        "                })\n",
        "            else:  # No defects\n",
        "                formatted.append({\n",
        "                    'boxes': torch.empty((0, 4), dtype=torch.float32).to(self.device),\n",
        "                    'labels': torch.empty((0,), dtype=torch.long).to(self.device),\n",
        "                    'masks': torch.empty((0, 224, 224), dtype=torch.uint8).to(self.device)\n",
        "                })\n",
        "        return formatted\n",
        "    \n",
        "    def _save_model(self, model_name: str, epoch: int):\n",
        "        \"\"\"Save model checkpoint\"\"\"\n",
        "        save_path = os.path.join(self.models_dir, f'{model_name}_epoch_{epoch}.pth')\n",
        "        torch.save(self.models[model_name].state_dict(), save_path)\n",
        "        print(f\"ðŸ’¾ Saved {model_name} checkpoint: {save_path}\")\n",
        "    \n",
        "    def save_final_models(self):\n",
        "        \"\"\"Save final trained models\"\"\"\n",
        "        for name, model in self.models.items():\n",
        "            save_path = os.path.join(self.models_dir, f'{name}_final.pth')\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            print(f\"ðŸ’¾ Saved final {name} model: {save_path}\")\n",
        "    \n",
        "    def plot_training_history(self):\n",
        "        \"\"\"Plot training history\"\"\"\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "        \n",
        "        # CNN metrics\n",
        "        axes[0, 0].plot(self.training_history['cnn_loss'])\n",
        "        axes[0, 0].set_title('CNN Loss')\n",
        "        axes[0, 0].set_xlabel('Epoch')\n",
        "        axes[0, 0].set_ylabel('Loss')\n",
        "        \n",
        "        axes[0, 1].plot(self.training_history['cnn_acc'])\n",
        "        axes[0, 1].set_title('CNN Accuracy')\n",
        "        axes[0, 1].set_xlabel('Epoch')\n",
        "        axes[0, 1].set_ylabel('Accuracy (%)')\n",
        "        \n",
        "        # Defect detector metrics\n",
        "        axes[0, 2].plot(self.training_history['defect_loss'])\n",
        "        axes[0, 2].set_title('Defect Detector Loss')\n",
        "        axes[0, 2].set_xlabel('Epoch')\n",
        "        axes[0, 2].set_ylabel('Loss')\n",
        "        \n",
        "        # LSTM metrics\n",
        "        axes[1, 0].plot(self.training_history['lstm_loss'])\n",
        "        axes[1, 0].set_title('LSTM Loss')\n",
        "        axes[1, 0].set_xlabel('Epoch')\n",
        "        axes[1, 0].set_ylabel('Loss')\n",
        "        \n",
        "        axes[1, 1].plot(self.training_history['lstm_mae'])\n",
        "        axes[1, 1].set_title('LSTM MAE')\n",
        "        axes[1, 1].set_xlabel('Epoch')\n",
        "        axes[1, 1].set_ylabel('MAE')\n",
        "        \n",
        "        # Hide empty subplot\n",
        "        axes[1, 2].set_visible(False)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.models_dir, 'training_history.png'))\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main training function\"\"\"\n",
        "    print(\"ðŸŽ¯ BeanScan Deep Learning Model Training\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Initialize trainer\n",
        "    trainer = ModelTrainer(device='cpu')  # Use 'cuda' if GPU available\n",
        "    \n",
        "    # Create dummy datasets (replace with real data)\n",
        "    train_dataset = BeanImageDataset('./data', split='train')\n",
        "    val_dataset = BeanImageDataset('./data', split='val')\n",
        "    \n",
        "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "    \n",
        "    print(f\"ðŸ“Š Dataset sizes: Train={len(train_dataset)}, Val={len(val_dataset)}\")\n",
        "    \n",
        "    # Train all models\n",
        "    try:\n",
        "        # Train CNN\n",
        "        cnn_loss, cnn_acc = trainer.train_cnn(train_loader, val_loader)\n",
        "        \n",
        "        # Train Defect Detector\n",
        "        defect_loss = trainer.train_defect_detector(train_loader, val_loader)\n",
        "        \n",
        "        # Train LSTM\n",
        "        lstm_loss, lstm_mae = trainer.train_lstm(train_loader, val_loader)\n",
        "        \n",
        "        # Save final models\n",
        "        trainer.save_final_models()\n",
        "        \n",
        "        # Plot training history\n",
        "        trainer.plot_training_history()\n",
        "        \n",
        "        print(\"\\nðŸŽ‰ All models trained successfully!\")\n",
        "        print(\"ðŸ“ Models saved in:\", trainer.models_dir)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Training failed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "'''\n",
        "\n",
        "with open(os.path.join(base_dir, 'custom_models.py'), 'w', encoding='utf-8') as f:\n",
        "    f.write(custom_models_src)\n",
        "with open(os.path.join(base_dir, 'train_models.py'), 'w', encoding='utf-8') as f:\n",
        "    f.write(train_models_src)\n",
        "\n",
        "import sys\n",
        "if '/kaggle/working/backend' not in sys.path:\n",
        "    sys.path.append('/kaggle/working/backend')\n",
        "if '/kaggle/working/backend/ml' not in sys.path:\n",
        "    sys.path.append('/kaggle/working/backend/ml')\n",
        "\n",
        "print('Wrote inline modules to', base_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Prepare code and paths ===\n",
        "import os, sys, shutil\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "\n",
        "# Copy code from attached dataset into the working dir for clean imports\n",
        "if os.path.exists(DATASET_CODE):\n",
        "    if os.path.exists('/kaggle/working/backend'):\n",
        "        shutil.rmtree('/kaggle/working/backend')\n",
        "    shutil.copytree(os.path.join(DATASET_CODE, 'backend'), '/kaggle/working/backend')\n",
        "else:\n",
        "    print(\"WARNING: DATASET_CODE not found. Ensure you attached your code dataset.\")\n",
        "\n",
        "# Add ml path to sys.path for imports\n",
        "sys.path.append('/kaggle/working/backend/ml')\n",
        "sys.path.append('/kaggle/working/backend')\n",
        "\n",
        "print(\"Python version:\")\n",
        "import sys as _sys; print(_sys.version)\n",
        "print(\"Torch version:\")\n",
        "import torch as _torch; print(_torch.__version__, 'CUDA:', _torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Patch training script defaults for Kaggle ===\n",
        "# We import ModelTrainer and BeanImageDataset, then override parameters/paths.\n",
        "from backend.ml.train_models import ModelTrainer, BeanImageDataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "\n",
        "# Choose device\n",
        "device = 'cuda' if (USE_GPU and torch.cuda.is_available()) else 'cpu'\n",
        "print('Using device:', device)\n",
        "\n",
        "# Build datasets from Kaggle input\n",
        "train_dataset = BeanImageDataset(DATASET_IMAGES, split='train')\n",
        "val_dataset = BeanImageDataset(DATASET_IMAGES, split='val')  # keep same dir structure\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=(device=='cuda'))\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=(device=='cuda'))\n",
        "\n",
        "# Initialize trainer and override epochs/save intervals and models_dir\n",
        "trainer = ModelTrainer(device=device, models_dir=MODELS_DIR)\n",
        "trainer.num_epochs = NUM_EPOCHS\n",
        "trainer.batch_size = BATCH_SIZE\n",
        "trainer.save_interval = SAVE_INTERVAL\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === (Optional) Resume from pretrained weights ===\n",
        "# If you attach a weights dataset OR a Kaggle Model, you can load weights here.\n",
        "# Examples:\n",
        "# - DATASET_WEIGHTS = \"/kaggle/input/beanscan-weights\" and it contains cnn_best.pth\n",
        "# - WEIGHTS_FILE = \"/kaggle/input/cnn-cnn-v1/cnn_best.pth\" from the Models panel\n",
        "import os, glob, torch\n",
        "\n",
        "loaded = False\n",
        "if WEIGHTS_FILE and os.path.exists(WEIGHTS_FILE):\n",
        "    print(f\"Loading CNN weights file: {WEIGHTS_FILE}\")\n",
        "    trainer.models['cnn'].load_state_dict(torch.load(WEIGHTS_FILE, map_location=device), strict=False)\n",
        "    loaded = True\n",
        "\n",
        "if (not loaded) and DATASET_WEIGHTS and os.path.exists(DATASET_WEIGHTS):\n",
        "    def try_load(model, pattern):\n",
        "        files = sorted(glob.glob(os.path.join(DATASET_WEIGHTS, pattern)))\n",
        "        if files:\n",
        "            path = files[-1]\n",
        "            print(f\"Loading weights: {path}\")\n",
        "            state = torch.load(path, map_location=device)\n",
        "            model.load_state_dict(state, strict=False)\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    loaded = try_load(trainer.models['cnn'], 'cnn*.pth') or loaded\n",
        "    _ = try_load(trainer.models['defect_detector'], 'defect*rcnn*.pth')\n",
        "    _ = try_load(trainer.models['lstm'], 'lstm*.pth')\n",
        "\n",
        "if not loaded:\n",
        "    print(\"No CNN weights loaded; training CNN from scratch.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Train (CNN only to start) ===\n",
        "cnn_loss, cnn_acc = trainer.train_cnn(train_loader, val_loader)\n",
        "\n",
        "# Save final CNN weights and training history plot\n",
        "trainer.save_final_models()\n",
        "trainer.plot_training_history()\n",
        "\n",
        "print(\"Models saved to:\", MODELS_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes\n",
        "- Place `train_annotations.json` and `val_annotations.json` in your images dataset root so `BeanImageDataset` can find them.\n",
        "- Images should be accessible by `image_id` inside that same root.\n",
        "- Outputs are written to `/kaggle/working/models`; save a notebook version to persist.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
